---
title: "main"
output: 
  html_document: 
    toc: true
    keep_md: true
---

```{r renv Startup, eval=FALSE, include=FALSE}
renv::activate()
```

```{r renv Restore, include=FALSE}
renv::restore(prompt = FALSE)
```

# 0. Reset Memory and Re-import libraries

```{r R setup, warning=FALSE, include=FALSE}
# 데이터를 모두 해제합니다.
rm(list=ls())

library("reticulate")
library("dplyr")
library("tidyr")
library("ggplot2")
library("tidytext")
library("stm")
library("tm")
library("pheatmap")
library("factoextra")
library("wordcloud")
library("rstudioapi")
library("jsonlite")

pymod<-import("modules")
# Plot export helpers
dataset_name <- tools::file_path_sans_ext(basename("data/한동수CPC6.csv"))
plot_output_dir <- file.path("output", dataset_name)
dir.create(plot_output_dir, recursive = TRUE, showWarnings = FALSE)

plot_tracker <- new.env(parent = emptyenv())
plot_tracker$count <- 0

sanitize_label <- function(label) {
  label <- gsub("[^0-9A-Za-z가-힣_\\-]+", "_", label)
  label <- gsub("_+", "_", label)
  label <- sub("^_+", "", label)
  label <- sub("_+$", "", label)
  if (label == "") label <- "plot"
  label
}

save_plot <- function(label = "plot") {
  if (is.null(dev.list())) {
    message(sprintf("No active plot to save for label '%s'", label))
    return(invisible(NULL))
  }
  last_plot <- recordPlot()
  plot_tracker$count <- plot_tracker$count + 1
  filename <- sprintf("%02d_%s.png", plot_tracker$count, sanitize_label(label))
  filepath <- file.path(plot_output_dir, filename)
  png(filename = filepath, width = 1600, height = 1200, res = 180)
  replayPlot(last_plot)
  dev.off()
  message(sprintf("Saved plot: %s", filepath))
}
```

# 1. Import dataset

```{r eval=FALSE}

list.files(path="data")

CURRENT_DIR <- dirname(getActiveDocumentContext()$path)
```

```{r, echo=FALSE}

pat_raw <- read.csv(
  paste0("data/",dataset_name,".csv"),
  fileEncoding     = "UTF-8",
  stringsAsFactors = FALSE
)
```

# 2. Preprocess VIA Python

이 청크에서 파이썬의 사전 정의된 모듈을 Import하고, 데이터프레임으로 전처리합니다.

```{r echo=FALSE}
pat_preprocessed <- pat_raw %>%
  rename(
    id             = applicationNumber,
    title          = inventionTitle,
    abstract       = abstractContent,
    #year           = applicationDate,
    #org_name       = applicantName,
  ) %>% 
  pymod$preprocess_df(tagger_type="okt",title_weight=1L,abstract_weight=1L)
  
corpus <- Corpus(pat_preprocessed$processed_text %>% VectorSource)
# build corpus
print("PREPROCESSING COMPLETE")
```

## 2.a. Merge with metadata

```{r echo=FALSE}

metadata_list <- fromJSON(paste0("data/",dataset_name,"_metadata.json"), simplifyVector = FALSE)

app_ids <- pat_raw$applicationNumber %>% as.character
match_indices <- match(app_ids, names(metadata_list))

extract_field <- function(idx, field) {
  entry <- metadata_list[[idx]]
  entry[[field]]
}

# Primary applicant (first in list)
pat_preprocessed$applicantName <- match_indices %>% 
  sapply(\(x){extract_field(x,"applicantName")[[1]] %>% as.character})

# All applicants (semicolon-separated)
pat_preprocessed$applicantName_all <- match_indices %>%
  sapply(\(x){extract_field(x,"applicantName") %>% paste(collapse=";")})

# Application date (convert YYYYMMDD -> Date)
pat_preprocessed$applicationDate <-match_indices %>% 
  sapply(\(x){extract_field(x,"applicationDate")}) %>% as.Date(format="%Y%m%d")

top_applicants <- pat_preprocessed$applicantName %>%
  table() %>%
  sort(decreasing = TRUE) %>%
  names() %>%
  head(10)

pat <- pat_preprocessed %>% mutate(
  applicantGroup = ifelse(applicantName %in% top_applicants, applicantName, "Other") %>% factor,
  applicationYear = as.integer(format(applicationDate, "%Y")),
)
head(pat[, c("id", "applicantName")], 5)
```

### 2.b. TDM/DTM

```{r echo=FALSE, warning=FALSE}

tdm<-TermDocumentMatrix(
  corpus,
  control = list(weighting=weightTfIdf)
)
tdm %>% inspect
```

```{r echo=FALSE, warning=FALSE}

TermDocumentMatrix(
  corpus,
  control=list(weightTf)
) %>% inspect
```

# 3. Use TextProcessor

```{r echo=FALSE}
proc <- textProcessor(
  documents = pat$processed_text,
  metadata = pat,
  lowercase= FALSE,
  removepunctuation = FALSE,
  removestopwords = FALSE,
  customstopwords = NULL,
  removenumbers = FALSE,
  stem=FALSE,
  wordLengths = c(1,Inf)
)

head(proc$vocab,3)
```

```{r}
prep<-prepDocuments(
  documents = proc$documents,
  vocab = proc$vocab,
  meta = proc$meta,
)
plotRemoved(prep$documents, lower.thresh = seq(0, 100, by = 5))
save_plot("documents_filtered")
```

```{r echo=FALSE}

corpus<-prepDocuments(
  documents = proc$documents,
  vocab = proc$vocab,
  meta = proc$meta,
  lower.thresh = 1
)

head(corpus$vocab,3)

docs <- corpus$documents
vocab <- corpus$vocab
meta <- corpus$meta
```

### 3.1.1 Print Top terms

```{r echo=FALSE}

#dec_srt <- prep$wordcounts
#dec_srt %>% sort(decreasing = TRUE)
#top_freq  = head(dec_srt %>% sort(decreasing = TRUE),10)
#
#token_counts <- docs %>%
#  unlist %>%                                             # convert large list to vector
#  table %>%                                              # table of occurence
#  sort(decreasing = TRUE)                                # sort table
#
#n_top <- min(token_counts %>% length,30)                 # length of table, restricted to 30
#seq <- seq_len(n_top)                                    # a sequence of integer, 1 to n_top
#
#top_idx <- (token_counts %>% names)[seq] %>% as.integer  # Indices of most occurent tokens as INT
#
#top_terms_stm <- data.frame(
#  term = vocab[top_idx],
#  n    = token_counts[seq] %>% as.integer
#)                                                        # make dataFrame 
#
#top_terms_stm  # print DataFrame
```

# 4. Search for optimum K

```{r echo=FALSE, include=True}

candidates <- seq(from=8, to=24, by=2)

kresult <- searchK(
  documents = docs,
  vocab = vocab,
  data = meta,
  K = candidates,
  seed = 42,
)

plot(kresult)
save_plot("searchK")
```

```{r echo=FALSE, message=FALSE}
K_final <- 16
```

# 5. Fit STM model

```{r echo=FALSE, fig.height=5, paged.print=TRUE}
set.seed(42)

stm_model <- stm(
  documents = docs,
  vocab = vocab,
  K = K_final,
  prevalence = ~ applicantGroup * applicationYear,
  data = meta,
  max.em.its = 200,
  init.type = "Spectral",
  verbose=TRUE,
  reportevery=5
)
```

# 6. Plots

## 6.1. STM Summary

```{r echo=FALSE}
plot(stm_model, type= "summary", text.cex = 0.8)
save_plot("stm_summary")
labelTopics(stm_model, topics = 1: K_final, n=10)

```

## 6.2. STM topicCorr Heatmap

```{r echo=FALSE}
stm_cor <- topicCorr(stm_model, cutoff = -0.01)
rownames(stm_cor$cor) <- colnames(stm_cor$cor) <- paste0("topic ", 1:K_final)

k_val <- 5

stm_cor$cor %>% 
    pheatmap(display_numbers=T, clustering_method ="ward.D2" ,cutree_rows=k_val, cutree_cols=k_val)
save_plot("topic_correlation_heatmap")
```

## 6.3. Dendrogram & Dendrogram-based Cluster Plot

```{r echo=FALSE,warning=FALSE}

k_val <- 5

stm_hc <- hclust(stm_cor$cor %>% dist(method="euclidean"), "ward.D2")
fviz_dend(stm_hc, k=k_val, palette="Dark2", rect=T)
save_plot("stm_dendrogram")

plot(stm_cor, vertex.color = stm_hc %>% cutree(k=k_val) %>% as.factor)
save_plot("stm_topic_graph")
```

## 6.4. K-meas based Cluster Plot

```{r eval=FALSE,echo=FALSE}

k_val <- 4

fviz_cluster(stm_cor$cor %>% kmeans(k_val), stm_cor$cor)
#save_plot("stm_kmeans_cluster")

```

## 6.5. Applicant-Year Interaction Effect

```{r Applicant Year Interaction, echo=FALSE, fig.width=12, fig.height=8}
if (exists("stm_model")) {
  effect_model <- estimateEffect(
    1:K_final ~ applicantGroup * applicationYear,
    stmobj = stm_model,
    metadata = meta,
    uncertainty = "Global"
  )

  topics_to_plot <- seq_len(K_final)
  target_applicants <- intersect(
    levels(pat$applicantGroup),
    c(top_applicants, "Other")
  ) %>% unique()
  #%>% head(3)

  if (length(target_applicants) == 0) {
    message("No applicant groups available for interaction plot.")
  } else {
    for (app in target_applicants) {
      plot(
        effect_model,
        covariate = "applicationYear",
        method = "continuous",
        moderator = "applicantGroup",
        moderator.value = app,
        topics = topics_to_plot,
        xlab = "Application Year",
        ylab = "Expected Topic Prevalence",
        main = paste("Applicant-Year Effect:", app),
        printlegend = TRUE,
        lwd = 3
      )
      #save_plot(paste0("interaction_", app))
    }
  }
} else {
  message("STM model not available; skipping interaction plot.")
}
```

# 7. Build Wordclouds

## 7.1. Topic Wordclouds

```{r echo=FALSE}

beta_mat <- exp(stm_model$beta$logbeta[[1]])

if (!exists("K_final")) {
  K_final <- stm_model$settings$dim$K
}

vocab_clean <- sapply(strsplit(stm_model$vocab, "/"), \(x) x[1])

for (k in 1:K_final) {
  topic_beta <- beta_mat[k, ]

  ord <- order(topic_beta, decreasing = TRUE)

  top_n <- 30
  top_idx <- ord[1:top_n]
  
  words_k <- vocab_clean[top_idx]
  freq_k  <- topic_beta[top_idx]
  
  # 워드클라우드
  wordcloud(
    words = words_k,
    freq = freq_k,
    max.words = top_n,
    scale = c(2, 0.6),
    random.order = FALSE,
  )
  
  title(paste("Topic", k, "Wordcloud"))
  save_plot(sprintf("topic_wordcloud_%02d", k))
}
```

## 7.2. Overall Wordcloud

```{r eval=FALSE}
# 문서-토픽 비중(gamma)
#gamma_df <- stm_model$theta %>% 
#  as.data.frame() %>% 
#  mutate(doc_id = 1:n())
#
#text_vec <- vapply(corpus, function(d) as.character(d), FUN.VALUE = character(1))
#text_df <- tibble(doc_id = seq_along(text_vec), text = text_vec)
#
## 토큰화 + TF-IDF 계산
#tfidf_df <- text_df %>%
#  unnest_tokens(word, text) %>%
#  count(doc_id, word, sort = TRUE) %>%
#  bind_tf_idf(word, doc_id, n)
#
## 전체 문서 기준 TF-IDF 상위 단어 선택
#top_terms <- tfidf_df %>%
#  arrange(desc(tf_idf)) %>%
#  slice_head(n = 100)
#
## 워드클라우드 생성
#wordcloud(
#  words = top_terms$word,
#  freq = top_terms$tf_idf,
#  max.words = 100,
#  scale = c(3, 0.8),
#  random.order = FALSE,
#  colors=rainbow(length(top_terms$word))
#)
#save_plot("overall_wordcloud")
```

# 8. Top Applicants by Topic Analysis

```{r Top Applicants Topic Analysis, echo=FALSE, fig.width=12, fig.height=8}
# Get topic assignments for each document
gamma_df <- stm_model$theta %>%
  as.data.frame() %>%
  mutate(doc_id = row_number())

# Find dominant topic for each document
topic_assignments <- gamma_df %>%
  pivot_longer(cols = -doc_id, names_to = "topic", values_to = "gamma") %>%
  group_by(doc_id) %>%
  slice_max(gamma, n = 1) %>%
  ungroup() %>%
  mutate(topic = gsub("V", "Topic ", topic))

# Merge with metadata to get applicant information
if (exists("pat") && "applicantName" %in% names(pat)) {
  doc_topic_applicant <- pat %>%
    mutate(doc_id = row_number()) %>%
    left_join(topic_assignments, by = "doc_id") %>%
    filter(!is.na(applicantName))  # Remove rows with missing applicant names
  
  # Find top applicants
  top_applicants <- doc_topic_applicant %>%
    count(applicantName, sort = TRUE) %>%
    slice_head(n = 10) %>%
    pull(applicantName)
  
  # Analyze topic distribution for top applicants
  applicant_topic_summary <- doc_topic_applicant %>%
    filter(applicantName %in% top_applicants) %>%
    count(applicantName, topic) %>%
    group_by(applicantName) %>%
    mutate(percentage = n / sum(n) * 100) %>%
    ungroup()
  
  # Create visualization
  top_applicants_plot <- ggplot(
    applicant_topic_summary,
    aes(x = reorder(applicantName, -n, sum), y = n, fill = topic)
  ) +
    geom_bar(stat = "identity", position = "stack") +
    coord_flip() +
    labs(
      title = "Top Applicants by Topic Distribution",
      x = "Applicant Name",
      y = "Number of Patents",
      fill = "Topic"
    ) +
    theme_minimal() +
    theme(
      legend.position = "right",
      axis.text.y = element_text(size = 10)
    )

  print(top_applicants_plot)
  save_plot("top_applicants_topic_distribution")
  
  # Print summary table
  summary_table <- applicant_topic_summary %>%
    group_by(applicantName) %>%
    summarise(total_patents = sum(n)) %>%
    arrange(desc(total_patents))

  print("Top 10 Most Active Applicants:")
  print(summary_table)
} else {
  print("Warning: applicantName column not found in pat")
  print("Available columns:")
  if (exists("pat")) {
    print(names(pat))
  }
}
```

# 9. Applicant Topic Profiles

```{r Applicant Topic Profiles, echo=FALSE, fig.width=12, fig.height=8}
if (exists("pat") && exists("stm_model")) {
  # Long format for topic assignment
  topic_long <- stm_model$theta %>%
    as.data.frame() %>%
    mutate(doc_id = row_number()) %>%
    pivot_longer(-doc_id, names_to = "topic", values_to = "gamma")

  dominant_topic <- topic_long %>%
    group_by(doc_id) %>%
    slice_max(gamma, n = 1, with_ties = FALSE) %>%
    ungroup()

  doc_topic_applicant <- pat %>%
    mutate(doc_id = row_number()) %>%
    left_join(dominant_topic, by = "doc_id") %>%
    filter(!is.na(applicantName), !is.na(topic))

  top_applicants <- doc_topic_applicant %>%
    count(applicantName, sort = TRUE) %>%
    slice_head(n = 10) %>%
    pull(applicantName)

  plot_data <- doc_topic_applicant %>%
    filter(applicantName %in% top_applicants) %>%
    count(applicantName, topic)

  if (nrow(plot_data) == 0) {
    message("No topic data available for selected applicants")
  } else {
    for (applicant in top_applicants) {
      applicant_df <- plot_data %>% filter(applicantName == applicant)
      if (nrow(applicant_df) == 0) next

      applicant_plot <- ggplot(applicant_df, aes(x = topic, y = n, fill = topic)) +
        geom_col(show.legend = FALSE) +
        labs(
          title = paste("Topic Distribution:", applicant),
          x = "Topic",
          y = "Patent Count"
        ) +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

      print(applicant_plot)
      save_plot(paste0("topic_profile_", applicant))
    }
  }
} else {
  message("Required data (pat, stm_model) not available.")
}
```

# 10. Topic Volume by Application Year

```{r Topic Volume by Year, echo=FALSE, fig.width=12, fig.height=8}
if (exists("stm_model")) {
  year_meta <- meta %>%
    mutate(doc_id = row_number()) %>%
    select(doc_id, applicationYear)

  topic_long <- stm_model$theta %>%
    as.data.frame() %>%
    mutate(doc_id = row_number()) %>%
    pivot_longer(-doc_id, names_to = "topic", values_to = "gamma")

  dominant_topic <- topic_long %>%
    group_by(doc_id) %>%
    slice_max(gamma, n = 1, with_ties = FALSE) %>%
    ungroup() %>%
    mutate(topic = gsub("V", "Topic ", topic))

  topic_year_df <- dominant_topic %>%
    left_join(year_meta, by = "doc_id") %>%
    filter(!is.na(applicationYear)) %>%
    count(applicationYear, topic)

  if (nrow(topic_year_df) == 0) {
    message("No application year information available for plotting")
  } else {
    topic_year_plot <- ggplot(topic_year_df, aes(x = applicationYear, y = n, color = topic)) +
    geom_line(linewidth = 1) +
    geom_point(size = 1) +
    labs(
      title = "Topic Volume by Application Year",
      x = "Application Year",
      y = "Number of Patents",
      color = "Topic"
    ) +
    theme_minimal()
    
    print(topic_year_plot)
    save_plot("topic_volume_by_year")
    
    year_cols <- topic_year_df$applicationYear %>% unique() %>% sort() %>% as.character()

    topic_year_pivot <- topic_year_df %>%
      mutate(applicationYear = as.character(applicationYear)) %>%
      pivot_wider(names_from = applicationYear, values_from = n, values_fill = 0) %>%
      mutate(topic = factor(topic, levels = sort(unique(topic)))) %>%
      arrange(topic)

    topic_year_mat <- topic_year_pivot %>%
      select(topic, all_of(year_cols))

    topic_labels <- topic_year_mat$topic %>% as.character()
    topic_year_mat <- topic_year_mat %>%
      select(-topic) %>%
      as.matrix()
    rownames(topic_year_mat) <- topic_labels

    pheatmap(
      topic_year_mat,
      cluster_rows = FALSE,
      cluster_cols = FALSE,
      angle_col = 45,
      display_numbers = TRUE,
      number_format="%d",
      main = "Topic Volume by Application Year"
    )
    save_plot("topic_volume_by_year_heatmap")
  }
} else {
  message("STM model not available; skipping topic-by-year plot.")
}
```

# 11. Plot Radar-chart via Python and export.

```{r fig.height=10}
# [R Code] 파이썬으로 보낼 데이터 준비 (유사도 정렬)

# 1. 덴드로그램 순서 가져오기 (이미 hc가 있다고 가정)
# hc <- hclust(dist(scale(model_20$beta$logbeta[[1]])), method="ward.D2")
ordered_indices <- stm_hc$order  # 유사한 것끼리 뭉친 순서

# 2. 토픽 비중 및 라벨 준비
topic_props <- colMeans(stm_model$theta)
# 라벨은 아까 만든 AI 요약이나 FREX 단어를 씁니다 (여기선 Topic 번호+주요단어)
labels <- labelTopics(stm_model, n=3)$frex
topic_labels <- apply(labels, 1, paste, collapse="/")
topic_labels <- paste0("T", 1:K_final, "\n", topic_labels) # T1\n단어/단어/단어

# 3. 데이터프레임 생성 (순서대로 정렬!)
df_radar <- data.frame(
  Topic_ID = paste0("Topic ", 1:K_final),
  Proportion = topic_props,
  Label = topic_labels,
  Order = 1:K_final # 원래 번호
)

# ★ 핵심: 덴드로그램 순서대로 행을 재배치합니다.
df_radar <- df_radar[ordered_indices, ]

radar_plot_path <- file.path(plot_output_dir, "topic_radar_chart.png")
pymod$radar_chart(df_radar, radar_plot_path)
# 4. 저장
#write.csv(df_radar, paste0("radar/",dataset_name,".csv"), row.names = FALSE)
#cat("레이더 차트용 데이터 저장 완료!")
```
