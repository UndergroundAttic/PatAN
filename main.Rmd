---
title: "main"
output: 
  html_document: 
    toc: true
    keep_md: true
---

```{r renv Startup, include=FALSE}
renv::activate()
```

```{r renv Restore, include=FALSE}
renv::restore(prompt = FALSE)
```

# 0. Reset Memory and Re-import libraries

```{r R setup, warning=FALSE, include=FALSE}
# 데이터를 모두 해제합니다.
rm(list=ls())

library("reticulate")
library("dplyr")
library("tidyr")
library("ggplot2")
library("tidytext")
library("stm")
library("tm")
library("pheatmap")
library("factoextra")
```

# 1. Import dataset

```{r echo=FALSE}

rm(list=ls())
list.files(path="data")

CURRENT_DIR <- dirname(rstudioapi::getActiveDocumentContext()$path)
```

```{r, echo=FALSE}

pat_raw <- read.csv(
  "data/통합_품질.csv",
  fileEncoding     = "UTF-8",
  stringsAsFactors = FALSE
)
```

# 2. Preprocess VIA Python

이 청크에서 파이썬의 사전 정의된 모듈을 Import하고, 데이터프레임으로 전처리합니다.

```{r echo=FALSE}
pymod<-import("modules")

pat <- pat_raw %>%
  rename(
    id             = applicationNumber,
    title          = inventionTitle,
    abstract       = abstractContent,
    #year           = applicationDate,
    #org_name       = applicantName,
  ) %>% 
  pymod$preprocess_df(tagger_type="Okt",title_weight=1L,abstract_weight=1L)

corpus <- Corpus(VectorSource(pat$processed_text))
# build corpus
print("PREPROCESSING COMPLETE")
```

### 2.a. TDM/DTM

```{r echo=FALSE}

TermDocumentMatrix(
  corpus,
  control = list(weighting=weightTfIdf)
) %>% inspect
```

```{r echo=FALSE, warning=FALSE}

TermDocumentMatrix(
  corpus,
  control=list(weightTf)
) %>% inspect
```

# 3. Use TextProcessor

```{r echo=FALSE}
corpus <- textProcessor(
  documents = pat$processed_text,
  metadata = pat,
  lowercase= FALSE,
  removepunctuation = FALSE,
  removestopwords = FALSE,
  customstopwords = NULL,
  removenumbers = FALSE,
  stem=FALSE,
  wordLengths = c(1,Inf)
)

head(corpus$vocab,3)
```

```{r}
prep<-prepDocuments(
  documents = corpus$documents,
  vocab = corpus$vocab,
  meta = corpus$meta,
)
plotRemoved(prep$documents, lower.thresh = seq(0, 100, by = 5))
```

```{r echo=FALSE}

corpus<-prepDocuments(
  documents = corpus$documents,
  vocab = corpus$vocab,
  meta = corpus$meta,
  lower.thresh = 10
)

head(corpus$vocab,3)

docs <- corpus$documents
vocab <- corpus$vocab
meta <- corpus$meta
```

### 3.1.1 Print Top terms

```{r echo=FALSE}

token_counts <- unlist(docs) %>%                         # convert large list to vector
  table %>%                                              # table of occurence
  sort(decreasing = TRUE)                                # sort table

n_top <- min(token_counts %>% length,30)                 # length of table, restricted to 30
seq <- seq_len(n_top)                                    # a sequence of integer, 1 to n_top

top_idx <- (token_counts %>% names)[seq] %>% as.integer  # Indices of most occurent tokens as INT

top_terms_stm <- data.frame(
  term = vocab[top_idx],
  n    = token_counts[seq_len(n_top)] %>% as.integer
)                                                        # make dataFrame 

top_terms_stm  # print DataFrame
```

# 4. Search for optimum K

```{r message=FALSE, include=FALSE, paged.print=TRUE}

candidates <- seq(from=10, to=30, by=1)

kresult <- searchK(
  documents = docs,
  vocab = vocab,
  data = meta,
  K = candidates,
  seed = 42,
)
```

```{r echo=False}

plot(kresult)
```

```{r include=FALSE}
K_final <- 18
```

# 5. Fit STM model

```{r echo=FALSE}
set.seed(42)

stm_model <- stm(
  documents = docs,
  vocab = vocab,
  K = K_final,
  data = meta,
  max.em.its = 200,
  init.type = "Spectral"
)
```

# 6. Plot STM

```{r echo=FALSE}
plot(stm_model, type= "summary", text.cex = 0.8)
labelTopics(stm_model, topics = 1: K_final, n=10)
```

```{r echo=FALSE}

stm_cor <- topicCorr(stm_model)
rownames(stm_cor$cor) <- colnames(stm_cor$cor) <- paste0("topic ", 1:K_final)

clust_num <- 5

make.dt(stm_model, meta=corpus$meta) %>% 
    select(grep("Topic", names(.))) %>% 
    cor() %>% 
    pheatmap(display_numbers=T, cutree_rows=clust_num, cutree_cols=clust_num)


stm_hc <- hclust(dist(scale(stm_cor$cor), method="euclidean"), "ward.D2")
fviz_dend(stm_hc, k=clust_num, palette="Dark2", rect=T)

plot(topicCorr(stm_model,cutoff = 0.05), vertex.color = as.factor(cutree(stm_hc, k=clust_num)))

scale(stm_cor$cor) %>% kmeans(clust_num) %>% fviz_cluster(scale(stm_cor$cor))
```
