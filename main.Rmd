---
title: "main"
output: 
  html_document: 
    toc: true
    keep_md: true
---

```{r renv Startup, include=FALSE}
renv::activate()
```

```{r renv Restore, include=FALSE}
renv::restore(prompt = FALSE)
```

# 0. Reset Memory and Re-import libraries

```{r R setup, warning=FALSE, include=FALSE}
# 데이터를 모두 해제합니다.
rm(list=ls())

library("reticulate")
library("dplyr")
library("tidyr")
library("ggplot2")
library("tidytext")
library("stm")
library("tm")
library("pheatmap")
library("factoextra")
library("wordcloud")
```

# 1. Import dataset

```{r echo=FALSE}

rm(list=ls())
list.files(path="data")

CURRENT_DIR <- dirname(rstudioapi::getActiveDocumentContext()$path)
```

```{r, echo=FALSE}

pat_raw <- read.csv(
  "data/산공_IPC.csv",
  fileEncoding     = "UTF-8",
  stringsAsFactors = FALSE
)
```

# 2. Preprocess VIA Python

이 청크에서 파이썬의 사전 정의된 모듈을 Import하고, 데이터프레임으로 전처리합니다.

```{r echo=FALSE}
pymod<-import("modules")

pat <- pat_raw %>%
  rename(
    id             = applicationNumber,
    title          = inventionTitle,
    abstract       = abstractContent,
    #year           = applicationDate,
    #org_name       = applicantName,
  ) %>% 
  pymod$preprocess_df(tagger_type="Hannanum",title_weight=3L,abstract_weight=2L)

corpus <- Corpus(pat$processed_text %>% VectorSource)
# build corpus
print("PREPROCESSING COMPLETE")
```

### 2.a. TDM/DTM

```{r echo=FALSE, warning=FALSE}

TermDocumentMatrix(
  corpus,
  control = list(weighting=weightTfIdf)
) %>% inspect
```

```{r echo=FALSE, warning=FALSE}

TermDocumentMatrix(
  corpus,
  control=list(weightTf)
) %>% inspect
```

# 3. Use TextProcessor

```{r echo=FALSE}
corpus <- textProcessor(
  documents = pat$processed_text,
  metadata = pat,
  lowercase= FALSE,
  removepunctuation = FALSE,
  removestopwords = FALSE,
  customstopwords = NULL,
  removenumbers = FALSE,
  stem=FALSE,
  wordLengths = c(1,Inf)
)

head(corpus$vocab,3)
```

```{r}
prep<-prepDocuments(
  documents = corpus$documents,
  vocab = corpus$vocab,
  meta = corpus$meta,
)
plotRemoved(prep$documents, lower.thresh = seq(0, 100, by = 5))
```

```{r echo=FALSE}

corpus<-prepDocuments(
  documents = corpus$documents,
  vocab = corpus$vocab,
  meta = corpus$meta,
  lower.thresh = 10
)

head(corpus$vocab,3)

docs <- corpus$documents
vocab <- corpus$vocab
meta <- corpus$meta
```

### 3.1.1 Print Top terms

```{r echo=FALSE}

token_counts <- unlist(docs) %>%                         # convert large list to vector
  table %>%                                              # table of occurence
  sort(decreasing = TRUE)                                # sort table

n_top <- min(token_counts %>% length,30)                 # length of table, restricted to 30
seq <- seq_len(n_top)                                    # a sequence of integer, 1 to n_top

top_idx <- (token_counts %>% names)[seq] %>% as.integer  # Indices of most occurent tokens as INT

top_terms_stm <- data.frame(
  term = vocab[top_idx],
  n    = token_counts[seq] %>% as.integer
)                                                        # make dataFrame 

top_terms_stm  # print DataFrame
```

# 4. Search for optimum K

```{r echo=FALSE, include=True}

candidates <- seq(from=10, to=30, by=1)

kresult <- searchK(
  documents = docs,
  vocab = vocab,
  data = meta,
  K = candidates,
  seed = 42,
)

plot(kresult)
```

```{r echo=FALSE, message=FALSE}
K_final <- 18
```

# 5. Fit STM model

```{r echo=FALSE, fig.height=5, paged.print=TRUE}
set.seed(42)

stm_model <- stm(
  documents = docs,
  vocab = vocab,
  K = K_final,
  data = meta,
  max.em.its = 200,
  init.type = "Spectral",
  verbose=TRUE,
  reportevery=5
)
```

# 6. Plots

## 6.1. STM Summary

```{r echo=FALSE}
plot(stm_model, type= "summary", text.cex = 0.8)
labelTopics(stm_model, topics = 1: K_final, n=10)

stm_cor <- topicCorr(stm_model, cutoff = -0.01)
rownames(stm_cor$cor) <- colnames(stm_cor$cor) <- paste0("topic ", 1:K_final)
```

## 6.2. STM topicCorr Heatmap

```{r echo=FALSE}
k_val <- 5

stm_cor$cor %>% 
    pheatmap(display_numbers=T, clustering_method ="ward.D2" ,cutree_rows=k_val, cutree_cols=k_val)
```

## 6.3. Dendrogram & Dendrogram-based Cluster Plot

```{r echo=FALSE,warning=FALSE}

k_val <- 5

stm_hc <- hclust(stm_cor$cor %>% dist(method="euclidean"), "ward.D2")
fviz_dend(stm_hc, k=k_val, palette="Dark2", rect=T)

plot(stm_cor, vertex.color = stm_hc %>% cutree(k=k_val) %>% as.factor)
```

## 6.4. K-meas based Cluster Plot

```{r echo=FALSE}

k_val <- 4

fviz_cluster(stm_cor$cor %>% kmeans(k_val), stm_cor$cor)

```

# 7. Build Wordclouds

## 7.1. Topic Wordclouds

```{r echo=FALSE}

beta_mat <- exp(stm_model$beta$logbeta[[1]])

if (!exists("K_final")) {
  K_final <- stm_model$settings$dim$K
}

vocab_clean <- sapply(strsplit(stm_model$vocab, "/"), \(x) x[1])

for (k in 1:K_final) {
  topic_beta <- beta_mat[k, ]

  ord <- order(topic_beta, decreasing = TRUE)

  top_n <- 30
  top_idx <- ord[1:top_n]
  
  words_k <- vocab_clean[top_idx]
  freq_k  <- topic_beta[top_idx]
  
  # 워드클라우드
  wordcloud(
    words = words_k,
    freq = freq_k,
    max.words = top_n,
    scale = c(2, 0.6),
    random.order = FALSE,
  )
  
  title(paste("Topic", k, "Wordcloud"))
}
```

